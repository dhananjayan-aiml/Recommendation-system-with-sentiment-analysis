{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Step 1: Installing Dask for Sentiment Analysis\n",
        "This command installs several Python libraries for parallel computing, machine learning, and natural language processing, including Dask with all optional dependencies, Dask-ML for scalable ML algorithms, the Transformers library for pre-trained NLP models, and PyTorch for neural network-based AI tasks."
      ],
      "metadata": {
        "id": "WumpoBOnm_Mv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iMP9VJ1nVgen",
        "outputId": "75b15ba1-700b-4434-ecf5-871f454e1933"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: dask[complete] in /usr/local/lib/python3.10/dist-packages (2023.8.1)\n",
            "Requirement already satisfied: dask-ml in /usr/local/lib/python3.10/dist-packages (2024.4.4)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.40.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.2.1+cu121)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from dask[complete]) (8.1.7)\n",
            "Requirement already satisfied: cloudpickle>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from dask[complete]) (2.2.1)\n",
            "Requirement already satisfied: fsspec>=2021.09.0 in /usr/local/lib/python3.10/dist-packages (from dask[complete]) (2023.6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from dask[complete]) (24.0)\n",
            "Requirement already satisfied: partd>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from dask[complete]) (1.4.1)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from dask[complete]) (6.0.1)\n",
            "Requirement already satisfied: toolz>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from dask[complete]) (0.12.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.13.0 in /usr/local/lib/python3.10/dist-packages (from dask[complete]) (7.1.0)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.10/dist-packages (from dask[complete]) (14.0.2)\n",
            "Requirement already satisfied: lz4>=4.3.2 in /usr/local/lib/python3.10/dist-packages (from dask[complete]) (4.3.3)\n",
            "Requirement already satisfied: dask-glm>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from dask-ml) (0.3.2)\n",
            "Requirement already satisfied: distributed>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from dask-ml) (2023.8.1)\n",
            "Requirement already satisfied: multipledispatch>=0.4.9 in /usr/local/lib/python3.10/dist-packages (from dask-ml) (1.0.0)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from dask-ml) (0.58.1)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from dask-ml) (1.25.2)\n",
            "Requirement already satisfied: pandas>=0.24.2 in /usr/local/lib/python3.10/dist-packages (from dask-ml) (2.0.3)\n",
            "Requirement already satisfied: scikit-learn>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from dask-ml) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from dask-ml) (1.11.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.14.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.4.127)\n",
            "Requirement already satisfied: sparse>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from dask-glm>=0.2.0->dask-ml) (0.15.1)\n",
            "Requirement already satisfied: locket>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from distributed>=2.4.0->dask-ml) (1.0.0)\n",
            "Requirement already satisfied: msgpack>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from distributed>=2.4.0->dask-ml) (1.0.8)\n",
            "Requirement already satisfied: psutil>=5.7.2 in /usr/local/lib/python3.10/dist-packages (from distributed>=2.4.0->dask-ml) (5.9.5)\n",
            "Requirement already satisfied: sortedcontainers>=2.0.5 in /usr/local/lib/python3.10/dist-packages (from distributed>=2.4.0->dask-ml) (2.4.0)\n",
            "Requirement already satisfied: tblib>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from distributed>=2.4.0->dask-ml) (3.0.0)\n",
            "Requirement already satisfied: tornado>=6.0.4 in /usr/local/lib/python3.10/dist-packages (from distributed>=2.4.0->dask-ml) (6.3.3)\n",
            "Requirement already satisfied: urllib3>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from distributed>=2.4.0->dask-ml) (2.0.7)\n",
            "Requirement already satisfied: zict>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from distributed>=2.4.0->dask-ml) (3.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=4.13.0->dask[complete]) (3.18.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->dask-ml) (0.41.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.2->dask-ml) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.2->dask-ml) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.2->dask-ml) (2024.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.2.0->dask-ml) (1.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.2.0->dask-ml) (3.5.0)\n",
            "Requirement already satisfied: bokeh>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from dask[complete]) (3.3.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1 in /usr/local/lib/python3.10/dist-packages (from bokeh>=2.4.2->dask[complete]) (1.2.1)\n",
            "Requirement already satisfied: pillow>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from bokeh>=2.4.2->dask[complete]) (9.4.0)\n",
            "Requirement already satisfied: xyzservices>=2021.09.1 in /usr/local/lib/python3.10/dist-packages (from bokeh>=2.4.2->dask[complete]) (2024.4.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=0.24.2->dask-ml) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install dask[complete] dask-ml transformers torch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Intialise Dask Clusters and Load the data\n",
        "Read the dataset from a file, which could be stored on your Google Drive:"
      ],
      "metadata": {
        "id": "bFsWNiB-nP81"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from dask.distributed import Client, LocalCluster\n",
        "\n",
        "# For local development\n",
        "cluster = LocalCluster(\n",
        "    n_workers=4,          # Number of workers\n",
        "    threads_per_worker=2, # Number of threads per each worker\n",
        "    memory_limit='2GB'    # Memory limit for each worker\n",
        ")\n",
        "client = Client(cluster)\n",
        "\n",
        "# Enable adaptive scaling\n",
        "cluster.adapt(minimum=2, maximum=10)\n",
        "\n",
        "client = Client(processes=False)  # Use processes=False if you're on a single machine for better debugging"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gLblRu-oVn-a",
        "outputId": "75f239a0-166e-439b-bcb1-9dbb12c425c6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:distributed.http.proxy:To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy\n",
            "INFO:distributed.scheduler:State start\n",
            "INFO:distributed.scheduler:  Scheduler at:     tcp://127.0.0.1:40917\n",
            "INFO:distributed.scheduler:  dashboard at:  http://127.0.0.1:8787/status\n",
            "INFO:distributed.nanny:        Start Nanny at: 'tcp://127.0.0.1:36659'\n",
            "INFO:distributed.nanny:        Start Nanny at: 'tcp://127.0.0.1:40233'\n",
            "INFO:distributed.nanny:        Start Nanny at: 'tcp://127.0.0.1:41793'\n",
            "INFO:distributed.nanny:        Start Nanny at: 'tcp://127.0.0.1:43923'\n",
            "INFO:distributed.scheduler:Register worker <WorkerState 'tcp://127.0.0.1:39085', name: 0, status: init, memory: 0, processing: 0>\n",
            "INFO:distributed.scheduler:Starting worker compute stream, tcp://127.0.0.1:39085\n",
            "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:34734\n",
            "INFO:distributed.scheduler:Register worker <WorkerState 'tcp://127.0.0.1:44791', name: 2, status: init, memory: 0, processing: 0>\n",
            "INFO:distributed.scheduler:Starting worker compute stream, tcp://127.0.0.1:44791\n",
            "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:34750\n",
            "INFO:distributed.scheduler:Register worker <WorkerState 'tcp://127.0.0.1:37969', name: 3, status: init, memory: 0, processing: 0>\n",
            "INFO:distributed.scheduler:Starting worker compute stream, tcp://127.0.0.1:37969\n",
            "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:34770\n",
            "INFO:distributed.scheduler:Register worker <WorkerState 'tcp://127.0.0.1:42481', name: 1, status: init, memory: 0, processing: 0>\n",
            "INFO:distributed.scheduler:Starting worker compute stream, tcp://127.0.0.1:42481\n",
            "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:34762\n",
            "INFO:distributed.scheduler:Receive client connection: Client-a9bb26cd-0a6e-11ef-a313-0242ac1c000c\n",
            "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:34782\n",
            "INFO:distributed.deploy.adaptive:Adaptive scaling started: minimum=2 maximum=10\n",
            "/usr/local/lib/python3.10/dist-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.\n",
            "Perhaps you already have a cluster running?\n",
            "Hosting the HTTP server on port 44997 instead\n",
            "  warnings.warn(\n",
            "INFO:distributed.scheduler:State start\n",
            "INFO:distributed.scheduler:  Scheduler at: inproc://172.28.0.12/123667/1\n",
            "INFO:distributed.scheduler:  dashboard at:  http://172.28.0.12:44997/status\n",
            "INFO:distributed.worker:      Start worker at: inproc://172.28.0.12/123667/4\n",
            "INFO:distributed.worker:         Listening to:          inproc172.28.0.12\n",
            "INFO:distributed.worker:          Worker name:                          0\n",
            "INFO:distributed.worker:         dashboard at:          172.28.0.12:44865\n",
            "INFO:distributed.worker:Waiting to connect to: inproc://172.28.0.12/123667/1\n",
            "INFO:distributed.worker:-------------------------------------------------\n",
            "INFO:distributed.worker:              Threads:                         12\n",
            "INFO:distributed.worker:               Memory:                  83.48 GiB\n",
            "INFO:distributed.worker:      Local Directory: /tmp/dask-scratch-space/worker-2o__nq76\n",
            "INFO:distributed.worker:-------------------------------------------------\n",
            "INFO:distributed.scheduler:Register worker <WorkerState 'inproc://172.28.0.12/123667/4', name: 0, status: init, memory: 0, processing: 0>\n",
            "INFO:distributed.scheduler:Starting worker compute stream, inproc://172.28.0.12/123667/4\n",
            "INFO:distributed.core:Starting established connection to inproc://172.28.0.12/123667/5\n",
            "INFO:distributed.worker:Starting Worker plugin shuffle\n",
            "INFO:distributed.worker:        Registered to: inproc://172.28.0.12/123667/1\n",
            "INFO:distributed.worker:-------------------------------------------------\n",
            "INFO:distributed.core:Starting established connection to inproc://172.28.0.12/123667/1\n",
            "INFO:distributed.scheduler:Receive client connection: Client-a9bcc1e0-0a6e-11ef-a313-0242ac1c000c\n",
            "INFO:distributed.core:Starting established connection to inproc://172.28.0.12/123667/6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1w7nyvvNWI4x",
        "outputId": "3d748994-e23e-4851-c8ee-a2560a8a255d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:distributed.deploy.adaptive:Retiring workers [1, 2]\n",
            "INFO:distributed.scheduler:Retire worker names (1, 2)\n",
            "INFO:distributed.scheduler:Retiring worker tcp://127.0.0.1:42481\n",
            "INFO:distributed.scheduler:Retiring worker tcp://127.0.0.1:44791\n",
            "INFO:distributed.active_memory_manager:Retiring worker tcp://127.0.0.1:44791; no unique keys need to be moved away.\n",
            "INFO:distributed.active_memory_manager:Retiring worker tcp://127.0.0.1:42481; no unique keys need to be moved away.\n",
            "INFO:distributed.scheduler:Remove worker <WorkerState 'tcp://127.0.0.1:42481', name: 1, status: closing_gracefully, memory: 0, processing: 0> (stimulus_id='retire-workers-1714865591.2226017')\n",
            "INFO:distributed.scheduler:Retired worker tcp://127.0.0.1:42481\n",
            "INFO:distributed.scheduler:Remove worker <WorkerState 'tcp://127.0.0.1:44791', name: 2, status: closing_gracefully, memory: 0, processing: 0> (stimulus_id='retire-workers-1714865591.2226017')\n",
            "INFO:distributed.scheduler:Retired worker tcp://127.0.0.1:44791\n",
            "INFO:distributed.nanny:Closing Nanny at 'tcp://127.0.0.1:40233'. Reason: nanny-close\n",
            "INFO:distributed.nanny:Nanny asking worker to close. Reason: nanny-close\n",
            "INFO:distributed.nanny:Closing Nanny at 'tcp://127.0.0.1:41793'. Reason: nanny-close\n",
            "INFO:distributed.nanny:Nanny asking worker to close. Reason: nanny-close\n",
            "INFO:distributed.core:Connection to tcp://127.0.0.1:34762 has been closed.\n",
            "INFO:distributed.core:Connection to tcp://127.0.0.1:34750 has been closed.\n",
            "WARNING:distributed.scheduler:Received heartbeat from unregistered worker 'tcp://127.0.0.1:44791'.\n",
            "WARNING:distributed.scheduler:Received heartbeat from unregistered worker 'tcp://127.0.0.1:42481'.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import dask.dataframe as dd\n",
        "\n",
        "# Define schema: Dask uses pandas for dtype, so define similarly\n",
        "schema = {\n",
        "    \"customer_id\": str,\n",
        "    \"product_id\": str,\n",
        "    \"product_parent\": str,\n",
        "    \"product_title\": str,\n",
        "    \"product_category\": str,\n",
        "    \"star_rating\": int,\n",
        "    \"full_text\": str,\n",
        "    \"language\": str\n",
        "}\n",
        "\n",
        "# Read the data with predefined schema\n",
        "df = dd.read_parquet('/content/drive/MyDrive/Big Data Project/language.parquet/', columns=list(schema.keys()))\n"
      ],
      "metadata": {
        "id": "iMubZN38VvFu"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Check if CUDA is available and then set the default device to GPU\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"Using GPU:\", torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"GPU not available, using CPU instead.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dSbW87i92Hvw",
        "outputId": "094d87d1-1e6e-4ac2-ce3c-5cba605cb234"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:distributed.nanny:Worker process still alive after 3.1999987792968754 seconds, killing\n",
            "WARNING:distributed.nanny:Worker process still alive after 3.199999389648438 seconds, killing\n",
            "INFO:distributed.nanny:Worker process 123800 was killed by signal 9\n",
            "INFO:distributed.nanny:Worker process 123803 was killed by signal 9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using GPU: NVIDIA A100-SXM4-40GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3 : Sentiment Analysis"
      ],
      "metadata": {
        "id": "erKF9bN_niHC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "# Pre-load models and tokenizers based on languages to avoid reloading them multiple times\n",
        "model_paths = {\n",
        "    \"en\": \"nlptown/bert-base-multilingual-uncased-sentiment\",\n",
        "    \"es\": \"dccuchile/bert-base-spanish-wwm-cased\",\n",
        "    \"fr\": \"camembert/camembert-large\",\n",
        "    \"de\": \"bert-base-german-dbmdz-uncased\",\n",
        "    \"zh\": \"bert-base-chinese\",\n",
        "    \"ar\": \"aubmindlab/bert-base-arabert\",\n",
        "    \"ru\": \"DeepPavlov/rubert-base-cased\",\n",
        "    \"pt\": \"neuralmind/bert-base-portuguese-cased\",\n",
        "    \"nl\": \"wietsedv/bert-base-dutch-cased\",\n",
        "    \"it\": \"dbmdz/bert-base-italian-xxl-cased\"\n",
        "}\n",
        "\n",
        "models = {lang: AutoModelForSequenceClassification.from_pretrained(path) for lang, path in model_paths.items()}\n",
        "tokenizers = {lang: AutoTokenizer.from_pretrained(path) for lang, path in model_paths.items()}\n",
        "\n",
        "def get_sentiment(texts, lang_codes):\n",
        "    results = []\n",
        "    for text, lang_code in zip(texts, lang_codes):\n",
        "        model = models.get(lang_code, models['en'])\n",
        "        tokenizer = tokenizers.get(lang_code, tokenizers['en'])\n",
        "        inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "        outputs = model(**inputs)\n",
        "        predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "        results.append(predictions.argmax(dim=-1).item())\n",
        "    return results\n",
        "\n",
        "# Use Dask's map_partitions to apply the function\n",
        "import pandas as pd\n",
        "\n",
        "def apply_sentiment(df):\n",
        "    df['sentiment'] = get_sentiment(df['full_text'].tolist(), df['language'].tolist())\n",
        "    return df\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z5D0NY-9WG72",
        "outputId": "2cea6977-b1b5-47c9-cd3b-19a0d8291611"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at camembert/camembert-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'roberta.embeddings.word_embeddings.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-german-dbmdz-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at aubmindlab/bert-base-arabert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at neuralmind/bert-base-portuguese-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at wietsedv/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-italian-xxl-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Correct meta information including all columns\n",
        "meta_df = {\n",
        "    \"customer_id\": str,\n",
        "    \"product_id\": str,\n",
        "    \"product_parent\": str,\n",
        "    \"product_title\": str,\n",
        "    \"product_category\": str,\n",
        "    \"star_rating\": int,\n",
        "    \"full_text\": str,\n",
        "    \"language\": str,\n",
        "    \"sentiment\": int  # New sentiment column as integer\n",
        "}\n",
        "\n",
        "result_df = df.map_partitions(apply_sentiment, meta=meta_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZzNpUv8FXVET",
        "outputId": "2d6644e6-a630-4891-f28b-b487d0ee4836"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:distributed.core:Event loop was unresponsive in Nanny for 118.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
            "INFO:distributed.core:Event loop was unresponsive in Scheduler for 117.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
            "INFO:distributed.core:Event loop was unresponsive in Worker for 117.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
            "INFO:distributed.core:Event loop was unresponsive in Scheduler for 117.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
            "INFO:distributed.core:Event loop was unresponsive in Nanny for 117.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result_df = result_df.compute()  # This will trigger the computation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6n8rJnuLVxRW",
        "outputId": "f2ac6f8d-64fe-420d-f921-c882817cb878"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/distributed/client.py:3160: UserWarning: Sending large graph of size 5.48 GiB.\n",
            "This may cause some slowdown.\n",
            "Consider scattering data ahead of time and using futures.\n",
            "  warnings.warn(\n",
            "INFO:distributed.core:Event loop was unresponsive in Scheduler for 9.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
            "INFO:distributed.core:Event loop was unresponsive in Worker for 9.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result_df.to_parquet('/content/drive/MyDrive/Big Data Project/senti.parquet')"
      ],
      "metadata": {
        "id": "AGzr_wXxiZQx"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(result_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7aamROYEkdMp",
        "outputId": "18d62e1a-3e0c-48c1-bffe-3032e54b425e"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     customer_id  product_id product_parent  \\\n",
            "0       32035145  B00CO2UY6C      546440076   \n",
            "1        9213870  B00WE2W2A8      319585048   \n",
            "2       12190192  B00NEZ6OW6      721363676   \n",
            "3        4176674  B00GJQN89O      934898207   \n",
            "4       22633251  B00AYZB6Z4      551463410   \n",
            "...          ...         ...            ...   \n",
            "2427    27014713  B000P6DYWA      911228139   \n",
            "2428    49034600  0743458117      859537711   \n",
            "2429      468675  B011THMEII      476091171   \n",
            "2430      588716  B000AUMYMM      306567904   \n",
            "2431    50849801  B0091UIMK0      750008325   \n",
            "\n",
            "                                          product_title product_category  \\\n",
            "0     Safavieh Lyndhurst Collection LNH214A Traditio...        Furniture   \n",
            "1     New Wayzon Mini Clip Metal Screen MP3 Music Me...      Electronics   \n",
            "2     Polaroid Cube HD 1080p Lifestyle Action Video ...           Camera   \n",
            "3     2 PINK Droplet Latex Free Blender Sponges Liqu...           Beauty   \n",
            "4                40 Inch LED Light Bar DR 14,400 Lumens       Automotive   \n",
            "...                                                 ...              ...   \n",
            "2427   Cuba Black by Cuba for Men - 3.3 Ounce EDT Spray           Beauty   \n",
            "2428  Alpine for You: A Passport to Peril Mystery (P...            Books   \n",
            "2429  S5 Case, Purple Dandelion Pattern Clear Soft T...          Apparel   \n",
            "2430  High Temperature High Gloss Self Leveling Brus...       Automotive   \n",
            "2431                   Jillian Michaels Body Revolution           Sports   \n",
            "\n",
            "      star_rating                                          full_text language  \\\n",
            "0             5.0                               Five Stars. Perfect!       en   \n",
            "1             1.0  This item just last for oneday.. This item jus...       en   \n",
            "2             2.0  5 minutes videos Max.. Although the camera is ...       en   \n",
            "3             1.0                 One Star. So disappointed on them😭       en   \n",
            "4             3.0  deal for the money, but you'll have do some wo...       en   \n",
            "...           ...                                                ...      ...   \n",
            "2427          4.0                                 Four Stars. thanks       en   \n",
            "2428          3.0  Mixed Review. I've read the series but have no...       en   \n",
            "2429          5.0  Make a Wish!. The graphic is so cute on this g...       en   \n",
            "2430          5.0           Great product!. Awesome and easy to use.       en   \n",
            "2431          4.0                          Four Stars. No complaints       en   \n",
            "\n",
            "      sentiment  \n",
            "0             4  \n",
            "1             0  \n",
            "2             1  \n",
            "3             0  \n",
            "4             2  \n",
            "...         ...  \n",
            "2427          3  \n",
            "2428          2  \n",
            "2429          4  \n",
            "2430          4  \n",
            "2431          3  \n",
            "\n",
            "[240000 rows x 9 columns]\n"
          ]
        }
      ]
    }
  ]
}